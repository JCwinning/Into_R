{
  "hash": "88ec8ef5f188f52ef22f8e4eb217e899",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"resample\"\n\nauthor: \"Tony Duan\"\n\nexecute:\n  warning: false\n  error: false\nformat:\n  html:\n    toc: true\n    toc-location: right\n    code-fold: show\n    code-tools: true\n    number-sections: false\n    code-block-bg: true\n    code-block-border-left: \"#31BAE9\"\n---\n\n\n# 1. The Importance of Resampling\n\nIn machine learning, we want to build models that generalize well to new, unseen data. A common mistake is to train and evaluate a model on the same dataset. This often leads to **overfitting**, where the model learns the noise in the training data, not the underlying pattern. As a result, the model performs well on the data it has seen but fails miserably on new data.\n\n**Resampling** is a collection of techniques used to combat this. It involves repeatedly drawing samples from a training set and refitting a model on each sample. This process allows us to:\n\n-   **Estimate model performance:** Get a more accurate and robust estimate of how the model will perform on unseen data.\n-   **Understand variability:** See how much the model's performance changes across different subsets of the data.\n-   **Tune hyperparameters:** Find the best settings for a model by evaluating them across multiple validation sets.\n\nThe `rsample` package, part of the `tidymodels` ecosystem, provides a consistent and intuitive interface for creating different types of resamples.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rsample)\nlibrary(tidyverse)\n\n# We will use the iris dataset for demonstration\ndata(iris)\n```\n:::\n\n# 2. k-Fold Cross-Validation (CV)\n\nThis is the most common resampling method. The data is randomly split into *k* equal-sized partitions (or \"folds\"). For each fold, the model is trained on the other *k-1* folds (the **analysis set**) and tested on the held-out fold (the **assessment set**). The process is repeated *k* times, with each fold serving as the test set exactly once. The final performance is the average of the performances across all *k* folds.\n\n![](images/three-CV-iter.svg){width=\"400\"}\n\n`vfold_cv()` creates the folds. A `v` of 10 is a common choice.\n\n::: {.cell}\n\n```{.r .cell-code}\nk_fold_resample <- vfold_cv(iris, v = 10)\nk_fold_resample\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation \n# A tibble: 10 × 2\n   splits           id    \n   <list>           <chr> \n 1 <split [135/15]> Fold01\n 2 <split [135/15]> Fold02\n 3 <split [135/15]> Fold03\n 4 <split [135/15]> Fold04\n 5 <split [135/15]> Fold05\n 6 <split [135/15]> Fold06\n 7 <split [135/15]> Fold07\n 8 <split [135/15]> Fold08\n 9 <split [135/15]> Fold09\n10 <split [135/15]> Fold10\n```\n\n\n:::\n:::\n\nEach row in the output represents one fold. The `<split>` column contains `rsplit` objects, which are lightweight pointers to the actual data for the training (analysis) and testing (assessment) sets for that fold.\n\n### Working with `rsplit` objects\n\nYou can extract the analysis and assessment data for a specific split.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get the first split object\nfirst_split <- k_fold_resample$splits[[1]]\n\n# Get the training data for the first fold\nanalysis_data <- analysis(first_split)\n\n# Get the testing data for the first fold\nassessment_data <- assessment(first_split)\n\ncat(paste(\"Analysis (training) rows:\", nrow(analysis_data), \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis (training) rows: 135 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(paste(\"Assessment (testing) rows:\", nrow(assessment_data), \"\\n\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAssessment (testing) rows: 15 \n```\n\n\n:::\n:::\n\n# 3. Monte Carlo Cross-Validation (MCCV)\n\nMCCV, also known as repeated random sub-sampling validation, involves randomly splitting the data into a training and a validation set a specified number of times. Unlike k-fold CV, the assessment sets are not mutually exclusive and can overlap.\n\nThis method is useful when you want to control the size of the validation set directly, rather than being constrained by the number of folds. It can be more computationally intensive than k-fold CV if many repetitions are used.\n\n`mc_cv()` creates the splits. `prop` is the proportion of data to be used for training, and `times` is the number of splits to create.\n\n::: {.cell}\n\n```{.r .cell-code}\nmc_resample <- mc_cv(iris, prop = 9/10, times = 20)\nmc_resample\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Monte Carlo cross-validation (0.9/0.1) with 20 resamples \n# A tibble: 20 × 2\n   splits           id        \n   <list>           <chr>     \n 1 <split [135/15]> Resample01\n 2 <split [135/15]> Resample02\n 3 <split [135/15]> Resample03\n 4 <split [135/15]> Resample04\n 5 <split [135/15]> Resample05\n 6 <split [135/15]> Resample06\n 7 <split [135/15]> Resample07\n 8 <split [135/15]> Resample08\n 9 <split [135/15]> Resample09\n10 <split [135/15]> Resample10\n11 <split [135/15]> Resample11\n12 <split [135/15]> Resample12\n13 <split [135/15]> Resample13\n14 <split [135/15]> Resample14\n15 <split [135/15]> Resample15\n16 <split [135/15]> Resample16\n17 <split [135/15]> Resample17\n18 <split [135/15]> Resample18\n19 <split [135/15]> Resample19\n20 <split [135/15]> Resample20\n```\n\n\n:::\n:::\n\n# 4. The Bootstrap\n\nA bootstrap sample is created by sampling from the original dataset **with replacement**. The resulting sample is the same size as the original dataset. Because of sampling with replacement, some data points will be selected multiple times, while others will not be selected at all (these are called the \"out-of-bag\" samples).\n\nBootstrapping is useful for estimating the uncertainty of a statistic (like a model coefficient) and is the foundation for ensemble methods like Random Forest. The out-of-bag sample can be used as a validation set.\n\n![](images/bootstraps.svg){width=\"495\"}\n\n`bootstraps()` creates the bootstrap samples.\n\n::: {.cell}\n\n```{.r .cell-code}\nbootstraps_resample <- bootstraps(iris, times = 1000)\nbootstraps_resample\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Bootstrap sampling \n# A tibble: 1,000 × 2\n   splits           id           \n   <list>           <chr>        \n 1 <split [150/59]> Bootstrap0001\n 2 <split [150/56]> Bootstrap0002\n 3 <split [150/56]> Bootstrap0003\n 4 <split [150/55]> Bootstrap0004\n 5 <split [150/59]> Bootstrap0005\n 6 <split [150/55]> Bootstrap0006\n 7 <split [150/55]> Bootstrap0007\n 8 <split [150/51]> Bootstrap0008\n 9 <split [150/55]> Bootstrap0009\n10 <split [150/57]> Bootstrap0010\n# ℹ 990 more rows\n```\n\n\n:::\n:::\n\n# 5. Stratified Sampling\n\nWhen dealing with a classification problem where the outcome variable is imbalanced (i.e., some classes have very few samples), random splitting can lead to folds where one or more classes are missing entirely from the training or testing set. \n\n**Stratified sampling** ensures that the class distribution in each fold is approximately the same as in the original dataset. You can do this by specifying the `strata` argument in the resampling functions. This is highly recommended for classification problems.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create 10-fold cross-validation splits, stratified by the Species column\nstratified_k_fold <- vfold_cv(iris, v = 10, strata = Species)\nstratified_k_fold\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits           id    \n   <list>           <chr> \n 1 <split [135/15]> Fold01\n 2 <split [135/15]> Fold02\n 3 <split [135/15]> Fold03\n 4 <split [135/15]> Fold04\n 5 <split [135/15]> Fold05\n 6 <split [135/15]> Fold06\n 7 <split [135/15]> Fold07\n 8 <split [135/15]> Fold08\n 9 <split [135/15]> Fold09\n10 <split [135/15]> Fold10\n```\n\n\n:::\n:::\n\n# 6. Visualizing Resamples\n\nIt can be helpful to visualize the resamples to understand how the data is being split. The code below defines a helper function to plot the distribution of a variable across the different resample splits.\n\n::: {.cell}\n\n```{.r .cell-code}\n# Helper function to extract and plot data from splits\nplot_resample_dist <- function(resample_obj, variable) {\n  resample_obj %>% \n    mutate(fold_id = as.character(row_number())) %>% \n    pull(splits) %>% \n    map_dfr(~as_tibble(analysis(.)), .id = \"fold_id\") %>% \n    ggplot(aes(x = {{variable}}, fill = fold_id)) + \n    geom_density(alpha = 0.5, show.legend = FALSE) + \n    labs(\n      title = paste(\"Distribution of\", rlang::as_name(enquo(variable)), \"across resamples\"),\n      x = rlang::as_name(enquo(variable)),\n      y = \"Density\"\n    ) + \n    theme_minimal()\n}\n\n# Visualize the distribution of Sepal.Length across the k-fold splits\nplot_resample_dist(k_fold_resample, Sepal.Length)\n```\n\n::: {.cell-output-display}\n![](5-resample_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\nThis visualization helps confirm that the splits are reasonably balanced and representative of the overall data distribution.\n\n\n",
    "supporting": [
      "5-resample_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}